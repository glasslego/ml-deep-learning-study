import numpy as np
import matplotlib.pyplot as plt

# í•œê¸€ í°íŠ¸ ì„¤ì • (MacOS ê¸°ì¤€)
plt.rcParams["font.family"] = "AppleGothic"

def generate_data():
    """ê°„ë‹¨í•œ ì„ í˜• ë°ì´í„° ìƒì„± (y = 4 + 3x + noise)"""
    np.random.seed(42)
    X = 2 * np.random.rand(100, 1)
    y = 4 + 3 * X + np.random.randn(100, 1)
    return X, y


# 1. Batch Gradient Descent
def batch_gradient_descent(X, y, learning_rate=0.01, iterations=1000):
    m = len(y)
    X_b = np.c_[np.ones((m, 1)), X]  # ì ˆí¸ ì¶”ê°€
    theta = np.random.randn(2, 1)  # íŒŒë¼ë¯¸í„° ì´ˆê¸°í™”

    history = []

    for i in range(iterations):
        gradients = 2 / m * X_b.T.dot(X_b.dot(theta) - y)
        theta = theta - learning_rate * gradients

        # ì†ì‹¤ ê³„ì‚°
        mse = np.mean((X_b.dot(theta) - y) ** 2)
        history.append(mse)

    return theta, history


# 2. Stochastic Gradient Descent (SGD)
def stochastic_gradient_descent(X, y, learning_rate=0.01, epochs=50):
    m = len(y)
    X_b = np.c_[np.ones((m, 1)), X]
    theta = np.random.randn(2, 1)

    history = []

    for epoch in range(epochs):
        for i in range(m):
            random_index = np.random.randint(m)
            xi = X_b[random_index:random_index + 1]
            yi = y[random_index:random_index + 1]

            gradients = 2 * xi.T.dot(xi.dot(theta) - yi)
            theta = theta - learning_rate * gradients

        # ì—í¬í¬ë§ˆë‹¤ ì†ì‹¤ ê³„ì‚°
        mse = np.mean((X_b.dot(theta) - y) ** 2)
        history.append(mse)

    return theta, history


# 3. Mini-batch Gradient Descent
def mini_batch_gradient_descent(X, y, learning_rate=0.01, epochs=50, batch_size=20):
    m = len(y)
    X_b = np.c_[np.ones((m, 1)), X]
    theta = np.random.randn(2, 1)

    history = []

    for epoch in range(epochs):
        shuffled_indices = np.random.permutation(m)
        X_b_shuffled = X_b[shuffled_indices]
        y_shuffled = y[shuffled_indices]

        for i in range(0, m, batch_size):
            xi = X_b_shuffled[i:i + batch_size]
            yi = y_shuffled[i:i + batch_size]

            gradients = 2 / batch_size * xi.T.dot(xi.dot(theta) - yi)
            theta = theta - learning_rate * gradients

        # ì—í¬í¬ë§ˆë‹¤ ì†ì‹¤ ê³„ì‚°
        mse = np.mean((X_b.dot(theta) - y) ** 2)
        history.append(mse)

    return theta, history


def run_gradient_descent_comparison():
    """Gradient Descent ì•Œê³ ë¦¬ì¦˜ ë¹„êµ ì‹¤í–‰"""
    print("=== Gradient Descent ì•Œê³ ë¦¬ì¦˜ ë¹„êµ ===")
    print()
    
    # ë°ì´í„° ìƒì„±
    X, y = generate_data()
    
    # ê° ì•Œê³ ë¦¬ì¦˜ ì‹¤í–‰
    theta_batch, history_batch = batch_gradient_descent(X, y, learning_rate=0.1, iterations=50)
    theta_sgd, history_sgd = stochastic_gradient_descent(X, y, learning_rate=0.01, epochs=50)
    theta_mini, history_mini = mini_batch_gradient_descent(X, y, learning_rate=0.1, epochs=50, batch_size=20)

    # ê²°ê³¼ ì¶œë ¥
    print("=== ê²°ê³¼ ë¹„êµ ===")
    print(f"ì‹¤ì œ ê°’: Î¸0 = 4, Î¸1 = 3")
    print(f"Batch GD:      Î¸0 = {theta_batch[0][0]:.4f}, Î¸1 = {theta_batch[1][0]:.4f}")
    print(f"SGD:           Î¸0 = {theta_sgd[0][0]:.4f}, Î¸1 = {theta_sgd[1][0]:.4f}")
    print(f"Mini-batch GD: Î¸0 = {theta_mini[0][0]:.4f}, Î¸1 = {theta_mini[1][0]:.4f}")
    print()

    # ì‹œê°í™”
    plt.figure(figsize=(12, 5))

    # ì†ì‹¤ í•¨ìˆ˜ ë¹„êµ
    plt.subplot(1, 2, 1)
    plt.plot(history_batch, label='Batch GD', linewidth=2)
    plt.plot(history_sgd, label='SGD', alpha=0.7)
    plt.plot(history_mini, label='Mini-batch GD', linewidth=2)
    plt.xlabel('Iterations/Epochs')
    plt.ylabel('MSE Loss')
    plt.title('ì†ì‹¤ í•¨ìˆ˜ ìˆ˜ë ´ ë¹„êµ')
    plt.legend()
    plt.grid(True, alpha=0.3)

    # ë°ì´í„°ì™€ ì˜ˆì¸¡ì„ 
    plt.subplot(1, 2, 2)
    plt.scatter(X, y, alpha=0.5, label='Data')
    X_new = np.array([[0], [2]])
    X_new_b = np.c_[np.ones((2, 1)), X_new]

    plt.plot(X_new, X_new_b.dot(theta_batch), 'r-', linewidth=2, label='Batch GD')
    plt.plot(X_new, X_new_b.dot(theta_sgd), 'g--', linewidth=2, label='SGD')
    plt.plot(X_new, X_new_b.dot(theta_mini), 'b:', linewidth=3, label='Mini-batch GD')
    plt.xlabel('X')
    plt.ylabel('y')
    plt.title('ì˜ˆì¸¡ ê²°ê³¼ ë¹„êµ')
    plt.legend()
    plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()


def main():
    """ë©”ì¸ í•¨ìˆ˜: Gradient Descent ì•Œê³ ë¦¬ì¦˜ ë¹„êµ ì‹¤í–‰"""
    print("ğŸ“Š Gradient Descent ì•Œê³ ë¦¬ì¦˜ ì™„ì „ ê°€ì´ë“œ ğŸ“Š")
    print("=" * 60)
    print()
    
    run_gradient_descent_comparison()
    
    print("=" * 60)
    print("ğŸ‰ Gradient Descent ë¹„êµê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ğŸ‰")
    print("=" * 60)


if __name__ == "__main__":
    main()