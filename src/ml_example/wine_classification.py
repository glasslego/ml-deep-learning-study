"""
ì‹¤ìŠµ 3: ì™€ì¸ í’ˆì§ˆ ë¶„ë¥˜ ì˜ˆì¸¡

ë¶„ë¥˜(Classification) ì•Œê³ ë¦¬ì¦˜ í•µì‹¬ ê°œë…:

1. ë¡œì§€ìŠ¤í‹± íšŒê·€ (Logistic Regression):
   - ì„ í˜• íšŒê·€ + ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ë¡œ í™•ë¥  ì˜ˆì¸¡
   - P(y=1|X) = 1 / (1 + e^(-Î²â‚€ - Î²â‚xâ‚ - ... - Î²â‚™xâ‚™))
   - ì†ì‹¤ í•¨ìˆ˜: ë¡œê·¸ ì†ì‹¤(Log Loss) = -Î£[y*log(p) + (1-y)*log(1-p)]
   - ì„ í˜• ê²°ì • ê²½ê³„, í™•ë¥ ì  ì¶œë ¥ ì œê³µ

2. ê²°ì • íŠ¸ë¦¬ (Decision Tree):
   - ê·œì¹™ ê¸°ë°˜ ë¶„í• ë¡œ ì˜ì‚¬ê²°ì • êµ¬ì¡° ìƒì„±
   - ë¶„í•  ê¸°ì¤€: ì§€ë‹ˆ ë¶ˆìˆœë„, ì—”íŠ¸ë¡œí”¼, ì •ë³´ ì´ë“
   - ì§€ë‹ˆ ë¶ˆìˆœë„ = 1 - Î£(páµ¢Â²), ì—”íŠ¸ë¡œí”¼ = -Î£(páµ¢ logâ‚‚ páµ¢)
   - ë¹„ì„ í˜• ê²°ì • ê²½ê³„, í•´ì„ ê°€ëŠ¥ì„± ìš°ìˆ˜

ë¶„ë¥˜ ì„±ëŠ¥ í‰ê°€ ì§€í‘œ:
- ì •í™•ë„(Accuracy): (TP + TN) / (TP + TN + FP + FN)
- ì •ë°€ë„(Precision): TP / (TP + FP) - ì–‘ì„± ì˜ˆì¸¡ ì¤‘ ì‹¤ì œ ì–‘ì„± ë¹„ìœ¨
- ì¬í˜„ìœ¨(Recall): TP / (TP + FN) - ì‹¤ì œ ì–‘ì„± ì¤‘ ì˜ˆì¸¡í•œ ì–‘ì„± ë¹„ìœ¨
- F1-Score: 2 * (Precision * Recall) / (Precision + Recall)
- ROC AUC: ë‹¤ì–‘í•œ ì„ê³„ê°’ì—ì„œì˜ ì„±ëŠ¥ ì¢…í•© í‰ê°€
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_wine
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    classification_report, confusion_matrix, accuracy_score,
    precision_score, recall_score, f1_score, roc_auc_score,
    roc_curve, precision_recall_curve
)
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.pipeline import Pipeline
import warnings
warnings.filterwarnings('ignore')

def load_and_explore_wine_data():
    """
    ì™€ì¸ ë°ì´í„°ì…‹ ë¡œë“œ ë° íƒìƒ‰ì  ë°ì´í„° ë¶„ì„
    
    Wine Dataset íŠ¹ì§•:
    - 3ê°œ í´ë˜ìŠ¤: Class 0, 1, 2 (ì™€ì¸ í’ˆì¢…)
    - 13ê°œ í”¼ì²˜: ì•Œì½”ì˜¬ ë„ìˆ˜, ë§ì‚°, ì• ì‰¬, ì•Œì¹¼ë¦¬ë„ ë“± í™”í•™ì  ì„±ë¶„
    - 178ê°œ ìƒ˜í”Œ, ê· í˜•ì¡íŒ í´ë˜ìŠ¤ ë¶„í¬
    """
    print("=== ì™€ì¸ ë°ì´í„°ì…‹ íƒìƒ‰ ===")
    
    # ë°ì´í„° ë¡œë“œ
    wine = load_wine()
    X, y = wine.data, wine.target
    
    # DataFrame ìƒì„±
    df = pd.DataFrame(X, columns=wine.feature_names)
    df['target'] = y
    df['target_name'] = df['target'].map({0: wine.target_names[0], 
                                         1: wine.target_names[1], 
                                         2: wine.target_names[2]})
    
    print(f"ë°ì´í„° í˜•íƒœ: {df.shape}")
    print(f"í”¼ì²˜ ìˆ˜: {len(wine.feature_names)}")
    print(f"í´ë˜ìŠ¤ ìˆ˜: {len(wine.target_names)}")
    print(f"í´ë˜ìŠ¤ëª…: {wine.target_names}")
    
    # í´ë˜ìŠ¤ ë¶„í¬ í™•ì¸
    print(f"\ní´ë˜ìŠ¤ ë¶„í¬:")
    class_counts = df['target'].value_counts().sort_index()
    for i, count in enumerate(class_counts):
        print(f"  {wine.target_names[i]} (Class {i}): {count}ê°œ ({count/len(df)*100:.1f}%)")
    
    # ê¸°ìˆ  í†µê³„
    print(f"\ní”¼ì²˜ë³„ ê¸°ìˆ  í†µê³„:")
    print(df[wine.feature_names].describe().round(2))
    
    # ê²°ì¸¡ê°’ í™•ì¸
    print(f"\nê²°ì¸¡ê°’: {df.isnull().sum().sum()}ê°œ")
    
    return X, y, df, wine

def visualize_wine_data(df, wine):
    """
    ì™€ì¸ ë°ì´í„° ì‹œê°í™”
    - í´ë˜ìŠ¤ë³„ í”¼ì²˜ ë¶„í¬ ë¹„êµ
    - ìƒê´€ê´€ê³„ ë¶„ì„
    - ì£¼ì„±ë¶„ ë¶„ì„ì„ í†µí•œ 2D ì‹œê°í™”
    """
    fig = plt.figure(figsize=(20, 15))
    
    # 1. í´ë˜ìŠ¤ ë¶„í¬
    plt.subplot(3, 4, 1)
    class_counts = df['target'].value_counts().sort_index()
    colors = ['skyblue', 'lightcoral', 'lightgreen']
    plt.bar(range(len(wine.target_names)), class_counts.values, color=colors)
    plt.xlabel('í´ë˜ìŠ¤')
    plt.ylabel('ìƒ˜í”Œ ìˆ˜')
    plt.title('í´ë˜ìŠ¤ ë¶„í¬')
    plt.xticks(range(len(wine.target_names)), wine.target_names, rotation=45)
    
    # 2. ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ
    plt.subplot(3, 4, 2)
    # í”¼ì²˜ ê°œìˆ˜ê°€ ë§ìœ¼ë¯€ë¡œ ìƒìœ„ 8ê°œ ì¤‘ìš” í”¼ì²˜ë§Œ ì„ íƒ
    important_features = ['alcohol', 'flavanoids', 'color_intensity', 'od280/od315_of_diluted_wines',
                         'proline', 'total_phenols', 'malic_acid', 'hue']
    if all(feature in df.columns for feature in important_features):
        corr_subset = df[important_features + ['target']].corr()
        sns.heatmap(corr_subset, annot=True, cmap='coolwarm', center=0, fmt='.2f', 
                   cbar_kws={'shrink': 0.8})
        plt.title('ì£¼ìš” í”¼ì²˜ ìƒê´€ê´€ê³„')
    
    # 3-6. ì£¼ìš” í”¼ì²˜ë³„ í´ë˜ìŠ¤ ë¶„í¬ (ë°•ìŠ¤í”Œë¡¯)
    key_features = ['alcohol', 'flavanoids', 'color_intensity', 'proline']
    for i, feature in enumerate(key_features):
        plt.subplot(3, 4, i+3)
        sns.boxplot(data=df, x='target', y=feature, palette=colors)
        plt.title(f'{feature} ë¶„í¬')
        plt.xlabel('í´ë˜ìŠ¤')
    
    # 7-10. ì£¼ìš” í”¼ì²˜ë³„ íˆìŠ¤í† ê·¸ë¨ (í´ë˜ìŠ¤ë³„)
    for i, feature in enumerate(key_features):
        plt.subplot(3, 4, i+7)
        for class_id in range(3):
            subset = df[df['target'] == class_id][feature]
            plt.hist(subset, alpha=0.6, label=f'{wine.target_names[class_id]}', 
                    color=colors[class_id], bins=15)
        plt.xlabel(feature)
        plt.ylabel('ë¹ˆë„')
        plt.title(f'{feature} íˆìŠ¤í† ê·¸ë¨')
        plt.legend()
    
    # 11. PCA 2D ì‹œê°í™”
    from sklearn.decomposition import PCA
    plt.subplot(3, 4, 11)
    
    # ë°ì´í„° í‘œì¤€í™” í›„ PCA
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(df[wine.feature_names])
    pca = PCA(n_components=2)
    X_pca = pca.fit_transform(X_scaled)
    
    for class_id in range(3):
        mask = df['target'] == class_id
        plt.scatter(X_pca[mask, 0], X_pca[mask, 1], 
                   c=colors[class_id], label=wine.target_names[class_id], 
                   alpha=0.7, s=50)
    
    plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} ë¶„ì‚°)')
    plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} ë¶„ì‚°)')
    plt.title(f'PCA ì‹œê°í™” (ì´ {pca.explained_variance_ratio_.sum():.1%} ë¶„ì‚° ì„¤ëª…)')
    plt.legend()
    
    # 12. í”¼ì²˜ ì¤‘ìš”ë„ (ìƒí˜¸ì •ë³´ëŸ‰)
    from sklearn.feature_selection import mutual_info_classif
    plt.subplot(3, 4, 12)
    
    mi_scores = mutual_info_classif(df[wine.feature_names], df['target'], random_state=42)
    feature_importance = pd.Series(mi_scores, index=wine.feature_names).sort_values(ascending=True)
    
    # ìƒìœ„ 10ê°œë§Œ í‘œì‹œ
    top_features = feature_importance.tail(10)
    top_features.plot(kind='barh', color='orange')
    plt.xlabel('ìƒí˜¸ì •ë³´ëŸ‰')
    plt.title('í”¼ì²˜ ì¤‘ìš”ë„ (ìƒìœ„ 10ê°œ)')
    plt.tight_layout()
    
    plt.savefig('wine_data_exploration.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    return X_pca, pca

def build_classification_models(X, y):
    """
    ì—¬ëŸ¬ ë¶„ë¥˜ ëª¨ë¸ êµ¬ì¶• ë° ë¹„êµ
    - ë¡œì§€ìŠ¤í‹± íšŒê·€: ì„ í˜• ë¶„ë¥˜ê¸°
    - ê²°ì • íŠ¸ë¦¬: ë¹„ì„ í˜•, í•´ì„ ê°€ëŠ¥
    - ëœë¤ í¬ë ˆìŠ¤íŠ¸: ì•™ìƒë¸”, ë†’ì€ ì„±ëŠ¥
    """
    print("\n=== ë¶„ë¥˜ ëª¨ë¸ êµ¬ì¶• ===")
    
    # ë°ì´í„° ë¶„í• 
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    print(f"í•™ìŠµ ë°ì´í„°: {X_train.shape}")
    print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {X_test.shape}")
    print(f"í•™ìŠµ ì„¸íŠ¸ í´ë˜ìŠ¤ ë¶„í¬: {np.bincount(y_train)}")
    print(f"í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ í´ë˜ìŠ¤ ë¶„í¬: {np.bincount(y_test)}")
    
    # ëª¨ë¸ ì •ì˜
    models = {
        'Logistic Regression': Pipeline([
            ('scaler', StandardScaler()),  # ë¡œì§€ìŠ¤í‹± íšŒê·€ëŠ” ìŠ¤ì¼€ì¼ë§ í•„ìˆ˜
            ('classifier', LogisticRegression(random_state=42, max_iter=1000))
        ]),
        'Decision Tree': DecisionTreeClassifier(
            random_state=42, 
            max_depth=5,  # ê³¼ì í•© ë°©ì§€
            min_samples_split=10,
            min_samples_leaf=5
        ),
        'Random Forest': RandomForestClassifier(
            n_estimators=100,
            random_state=42,
            max_depth=5,
            min_samples_split=10
        )
    }
    
    results = {}
    
    for name, model in models.items():
        print(f"\n--- {name} ---")
        
        # ëª¨ë¸ í•™ìŠµ
        model.fit(X_train, y_train)
        
        # ì˜ˆì¸¡
        y_train_pred = model.predict(X_train)
        y_test_pred = model.predict(X_test)
        
        # í™•ë¥  ì˜ˆì¸¡ (ROC ì»¤ë¸Œìš©)
        if hasattr(model, 'predict_proba'):
            y_test_proba = model.predict_proba(X_test)
        elif hasattr(model.named_steps['classifier'], 'predict_proba'):
            y_test_proba = model.predict_proba(X_test)
        else:
            y_test_proba = None
        
        # ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
        train_accuracy = accuracy_score(y_train, y_train_pred)
        test_accuracy = accuracy_score(y_test, y_test_pred)
        
        # ë‹¤ì¤‘ í´ë˜ìŠ¤ì˜ ê²½ìš° macro í‰ê·  ì‚¬ìš©
        test_precision = precision_score(y_test, y_test_pred, average='macro')
        test_recall = recall_score(y_test, y_test_pred, average='macro')
        test_f1 = f1_score(y_test, y_test_pred, average='macro')
        
        # ROC AUC (ë‹¤ì¤‘ í´ë˜ìŠ¤ì˜ ê²½ìš° ovr ë°©ì‹)
        if y_test_proba is not None:
            test_auc = roc_auc_score(y_test, y_test_proba, multi_class='ovr')
        else:
            test_auc = None
        
        # êµì°¨ê²€ì¦ (ë” robustí•œ í‰ê°€)
        cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')
        
        results[name] = {
            'model': model,
            'train_accuracy': train_accuracy,
            'test_accuracy': test_accuracy,
            'test_precision': test_precision,
            'test_recall': test_recall,
            'test_f1': test_f1,
            'test_auc': test_auc,
            'cv_mean': cv_scores.mean(),
            'cv_std': cv_scores.std(),
            'y_test_pred': y_test_pred,
            'y_test_proba': y_test_proba
        }
        
        print(f"  í•™ìŠµ ì •í™•ë„: {train_accuracy:.4f}")
        print(f"  í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_accuracy:.4f}")
        print(f"  ì •ë°€ë„: {test_precision:.4f}")
        print(f"  ì¬í˜„ìœ¨: {test_recall:.4f}")
        print(f"  F1 ìŠ¤ì½”ì–´: {test_f1:.4f}")
        if test_auc:
            print(f"  ROC AUC: {test_auc:.4f}")
        print(f"  CV ì •í™•ë„: {cv_scores.mean():.4f} (Â±{cv_scores.std():.4f})")
        print(f"  ê³¼ì í•© ì •ë„: {train_accuracy - test_accuracy:.4f}")
    
    return results, (X_test, y_test)

def analyze_confusion_matrices(results, y_test, wine):
    """
    í˜¼ë™ í–‰ë ¬(Confusion Matrix) ë¶„ì„
    - ê° í´ë˜ìŠ¤ë³„ ì˜ˆì¸¡ ì„±ëŠ¥ ìƒì„¸ ë¶„ì„
    - ì–´ë–¤ í´ë˜ìŠ¤ê°€ í˜¼ë™ë˜ê¸° ì‰¬ìš´ì§€ íŒŒì•…
    """
    print("\n=== í˜¼ë™ í–‰ë ¬ ë¶„ì„ ===")
    
    fig, axes = plt.subplots(1, len(results), figsize=(5*len(results), 4))
    if len(results) == 1:
        axes = [axes]
    
    for idx, (name, result) in enumerate(results.items()):
        cm = confusion_matrix(y_test, result['y_test_pred'])
        
        # í˜¼ë™ í–‰ë ¬ ì‹œê°í™”
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                   xticklabels=wine.target_names, 
                   yticklabels=wine.target_names,
                   ax=axes[idx])
        axes[idx].set_title(f'{name}\ní˜¼ë™ í–‰ë ¬')
        axes[idx].set_xlabel('ì˜ˆì¸¡ í´ë˜ìŠ¤')
        axes[idx].set_ylabel('ì‹¤ì œ í´ë˜ìŠ¤')
        
        # í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ë¶„ì„
        print(f"\n--- {name} í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ---")
        print(classification_report(y_test, result['y_test_pred'], 
                                  target_names=wine.target_names))
    
    plt.tight_layout()
    plt.savefig('confusion_matrices.png', dpi=300, bbox_inches='tight')
    plt.show()

def plot_roc_curves(results, y_test, wine):
    """
    ROC ì»¤ë¸Œ ë¶„ì„
    - ê° ëª¨ë¸ì˜ TPR vs FPR ì„±ëŠ¥ ì‹œê°í™”
    - ë‹¤ì¤‘ í´ë˜ìŠ¤ì˜ ê²½ìš° ê° í´ë˜ìŠ¤ë³„ ROC ì»¤ë¸Œ
    """
    print("\n=== ROC ì»¤ë¸Œ ë¶„ì„ ===")
    
    from sklearn.preprocessing import label_binarize
    from itertools import cycle
    
    # ë‹¤ì¤‘ í´ë˜ìŠ¤ë¥¼ ì´ì§„ ë¶„ë¥˜ë¡œ ë³€í™˜
    y_test_bin = label_binarize(y_test, classes=[0, 1, 2])
    n_classes = y_test_bin.shape[1]
    
    # ìƒ‰ìƒ ì„¤ì •
    colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])
    
    plt.figure(figsize=(15, 5))
    
    for idx, (name, result) in enumerate(results.items()):
        if result['y_test_proba'] is None:
            continue
            
        plt.subplot(1, len([r for r in results.values() if r['y_test_proba'] is not None]), idx+1)
        
        # ê° í´ë˜ìŠ¤ë³„ ROC ì»¤ë¸Œ
        for i, color in zip(range(n_classes), colors):
            fpr, tpr, _ = roc_curve(y_test_bin[:, i], result['y_test_proba'][:, i])
            auc = roc_auc_score(y_test_bin[:, i], result['y_test_proba'][:, i])
            
            plt.plot(fpr, tpr, color=color, lw=2,
                    label=f'{wine.target_names[i]} (AUC = {auc:.2f})')
        
        # ëŒ€ê°ì„  (ëœë¤ ë¶„ë¥˜ê¸°)
        plt.plot([0, 1], [0, 1], 'k--', lw=2, alpha=0.5)
        
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('ìœ„ì–‘ì„±ë¥  (False Positive Rate)')
        plt.ylabel('ë¯¼ê°ë„ (True Positive Rate)')
        plt.title(f'{name} ROC ì»¤ë¸Œ')
        plt.legend(loc="lower right")
    
    plt.tight_layout()
    plt.savefig('roc_curves.png', dpi=300, bbox_inches='tight')
    plt.show()

def analyze_decision_tree(results, wine):
    """
    ê²°ì • íŠ¸ë¦¬ ëª¨ë¸ í•´ì„
    - íŠ¸ë¦¬ êµ¬ì¡° ì‹œê°í™”
    - í”¼ì²˜ ì¤‘ìš”ë„ ë¶„ì„
    - ì˜ì‚¬ê²°ì • ê·œì¹™ ì¶”ì¶œ
    """
    if 'Decision Tree' not in results:
        return
    
    print("\n=== ê²°ì • íŠ¸ë¦¬ ë¶„ì„ ===")
    
    dt_model = results['Decision Tree']['model']
    
    # í”¼ì²˜ ì¤‘ìš”ë„
    feature_importance = pd.Series(
        dt_model.feature_importances_, 
        index=[f'feature_{i}' for i in range(len(dt_model.feature_importances_))]
    ).sort_values(ascending=False)
    
    print("í”¼ì²˜ ì¤‘ìš”ë„ (ìƒìœ„ 10ê°œ):")
    for feature, importance in feature_importance.head(10).items():
        print(f"  {feature}: {importance:.4f}")
    
    # ì‹œê°í™”
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))
    
    # 1. í”¼ì²˜ ì¤‘ìš”ë„ ë°” ì°¨íŠ¸
    feature_importance.head(10).plot(kind='barh', ax=ax1, color='skyblue')
    ax1.set_title('ê²°ì • íŠ¸ë¦¬ í”¼ì²˜ ì¤‘ìš”ë„')
    ax1.set_xlabel('ì¤‘ìš”ë„')
    
    # 2. íŠ¸ë¦¬ êµ¬ì¡° ì‹œê°í™” (ìµœëŒ€ ê¹Šì´ 3ê¹Œì§€ë§Œ)
    plot_tree(dt_model, max_depth=3, 
             feature_names=[f'F{i}' for i in range(len(dt_model.feature_importances_))],
             class_names=wine.target_names,
             filled=True, rounded=True, fontsize=10, ax=ax2)
    ax2.set_title('ê²°ì • íŠ¸ë¦¬ êµ¬ì¡° (ê¹Šì´ 3ê¹Œì§€)')
    
    plt.tight_layout()
    plt.savefig('decision_tree_analysis.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # íŠ¸ë¦¬ í†µê³„
    print(f"\níŠ¸ë¦¬ í†µê³„:")
    print(f"  ìµœëŒ€ ê¹Šì´: {dt_model.get_depth()}")
    print(f"  ë¦¬í”„ ë…¸ë“œ ìˆ˜: {dt_model.get_n_leaves()}")
    print(f"  ì´ ë…¸ë“œ ìˆ˜: {dt_model.tree_.node_count}")

def hyperparameter_tuning(X, y):
    """
    í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
    - Grid Searchë¥¼ í†µí•œ ìµœì  íŒŒë¼ë¯¸í„° íƒìƒ‰
    - êµì°¨ê²€ì¦ìœ¼ë¡œ ì„±ëŠ¥ í‰ê°€
    """
    print("\n=== í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ===")
    
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    # Decision Tree íŠœë‹
    print("Decision Tree íŠœë‹ ì¤‘...")
    dt_params = {
        'max_depth': [3, 5, 7, 10, None],
        'min_samples_split': [2, 5, 10, 20],
        'min_samples_leaf': [1, 2, 5, 10],
        'criterion': ['gini', 'entropy']
    }
    
    dt_grid = GridSearchCV(
        DecisionTreeClassifier(random_state=42),
        dt_params, cv=5, scoring='accuracy', n_jobs=-1
    )
    dt_grid.fit(X_train, y_train)
    
    print(f"ìµœì  íŒŒë¼ë¯¸í„°: {dt_grid.best_params_}")
    print(f"ìµœì  CV ì ìˆ˜: {dt_grid.best_score_:.4f}")
    
    # Random Forest íŠœë‹
    print("\nRandom Forest íŠœë‹ ì¤‘...")
    rf_params = {
        'n_estimators': [50, 100, 200],
        'max_depth': [3, 5, 7, 10],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 5]
    }
    
    rf_grid = GridSearchCV(
        RandomForestClassifier(random_state=42),
        rf_params, cv=5, scoring='accuracy', n_jobs=-1
    )
    rf_grid.fit(X_train, y_train)
    
    print(f"ìµœì  íŒŒë¼ë¯¸í„°: {rf_grid.best_params_}")
    print(f"ìµœì  CV ì ìˆ˜: {rf_grid.best_score_:.4f}")
    
    # íŠœë‹ëœ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ
    tuned_models = {
        'Tuned Decision Tree': dt_grid.best_estimator_,
        'Tuned Random Forest': rf_grid.best_estimator_
    }
    
    print("\níŠœë‹ í›„ ì„±ëŠ¥:")
    for name, model in tuned_models.items():
        model.fit(X_train, y_train)
        train_score = model.score(X_train, y_train)
        test_score = model.score(X_test, y_test)
        print(f"{name}:")
        print(f"  í•™ìŠµ ì •í™•ë„: {train_score:.4f}")
        print(f"  í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_score:.4f}")
    
    return dt_grid.best_estimator_, rf_grid.best_estimator_

def performance_comparison_summary(results):
    """
    ëª¨ë¸ ì„±ëŠ¥ ì¢…í•© ë¹„êµ
    """
    print("\n=== ëª¨ë¸ ì„±ëŠ¥ ì¢…í•© ë¹„êµ ===")
    
    # ì„±ëŠ¥ ë°ì´í„° ì •ë¦¬
    performance_data = []
    for name, result in results.items():
        performance_data.append({
            'Model': name,
            'Test Accuracy': result['test_accuracy'],
            'Precision': result['test_precision'],
            'Recall': result['test_recall'],
            'F1-Score': result['test_f1'],
            'AUC': result['test_auc'] if result['test_auc'] else 0,
            'CV Mean': result['cv_mean'],
            'CV Std': result['cv_std'],
            'Overfitting': result['train_accuracy'] - result['test_accuracy']
        })
    
    df_performance = pd.DataFrame(performance_data)
    print("\nì„±ëŠ¥ ë¹„êµí‘œ:")
    print(df_performance.round(4))
    
    # ì‹œê°í™”
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # 1. ì£¼ìš” ì§€í‘œ ë¹„êµ (ë°©ì‚¬í˜• ì°¨íŠ¸)
    ax = axes[0, 0]
    metrics = ['Test Accuracy', 'Precision', 'Recall', 'F1-Score']
    
    angles = np.linspace(0, 2*np.pi, len(metrics), endpoint=False).tolist()
    angles += angles[:1]  # ì›í˜• ì™„ì„±
    
    for _, row in df_performance.iterrows():
        values = [row[metric] for metric in metrics]
        values += values[:1]  # ì›í˜• ì™„ì„±
        ax.plot(angles, values, 'o-', linewidth=2, label=row['Model'])
        ax.fill(angles, values, alpha=0.1)
    
    ax.set_xticks(angles[:-1])
    ax.set_xticklabels(metrics)
    ax.set_ylim(0, 1)
    ax.set_title('ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ (ë°©ì‚¬í˜• ì°¨íŠ¸)')
    ax.legend()
    ax.grid(True)
    
    # 2. ì •í™•ë„ ë¹„êµ
    axes[0, 1].bar(df_performance['Model'], df_performance['Test Accuracy'], color='lightblue')
    axes[0, 1].set_title('í…ŒìŠ¤íŠ¸ ì •í™•ë„')
    axes[0, 1].set_ylabel('ì •í™•ë„')
    axes[0, 1].tick_params(axis='x', rotation=45)
    
    # 3. êµì°¨ê²€ì¦ ì ìˆ˜ (ì—ëŸ¬ë°” í¬í•¨)
    axes[1, 0].bar(df_performance['Model'], df_performance['CV Mean'],
                   yerr=df_performance['CV Std'], capsize=5, color='lightgreen')
    axes[1, 0].set_title('êµì°¨ê²€ì¦ ì •í™•ë„')
    axes[1, 0].set_ylabel('ì •í™•ë„')
    axes[1, 0].tick_params(axis='x', rotation=45)
    
    # 4. ê³¼ì í•© ì •ë„
    colors = ['red' if x > 0.05 else 'green' for x in df_performance['Overfitting']]
    axes[1, 1].bar(df_performance['Model'], df_performance['Overfitting'], color=colors)
    axes[1, 1].axhline(y=0.05, color='red', linestyle='--', alpha=0.7)
    axes[1, 1].set_title('ê³¼ì í•© ì •ë„')
    axes[1, 1].set_ylabel('í•™ìŠµ ì •í™•ë„ - í…ŒìŠ¤íŠ¸ ì •í™•ë„')
    axes[1, 1].tick_params(axis='x', rotation=45)
    
    plt.tight_layout()
    plt.savefig('classification_performance_summary.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸
    best_model = df_performance.loc[df_performance['Test Accuracy'].idxmax()]
    print(f"\nğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model['Model']}")
    print(f"   í…ŒìŠ¤íŠ¸ ì •í™•ë„: {best_model['Test Accuracy']:.4f}")
    print(f"   F1 ìŠ¤ì½”ì–´: {best_model['F1-Score']:.4f}")

def main():
    """
    ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
    """
    print("ğŸ· ì™€ì¸ ë¶„ë¥˜ ì˜ˆì¸¡ - ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜")
    print("=" * 60)
    
    # 1. ë°ì´í„° ë¡œë“œ ë° íƒìƒ‰
    X, y, df, wine = load_and_explore_wine_data()
    
    # 2. ë°ì´í„° ì‹œê°í™”
    X_pca, pca = visualize_wine_data(df, wine)
    
    # 3. ë¶„ë¥˜ ëª¨ë¸ êµ¬ì¶•
    results, test_data = build_classification_models(X, y)
    X_test, y_test = test_data
    
    # 4. í˜¼ë™ í–‰ë ¬ ë¶„ì„
    analyze_confusion_matrices(results, y_test, wine)
    
    # 5. ROC ì»¤ë¸Œ ë¶„ì„
    plot_roc_curves(results, y_test, wine)
    
    # 6. ê²°ì • íŠ¸ë¦¬ ë¶„ì„
    analyze_decision_tree(results, wine)
    
    # 7. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
    best_dt, best_rf = hyperparameter_tuning(X, y)
    
    # 8. ì„±ëŠ¥ ì¢…í•© ë¹„êµ
    performance_comparison_summary(results)
    
    print("\n" + "=" * 60)
    print("âœ… ë¶„ì„ ì™„ë£Œ!")
    print("ğŸ“Š ìƒì„±ëœ ì‹œê°í™” íŒŒì¼:")
    print("   - wine_data_exploration.png")
    print("   - confusion_matrices.png")
    print("   - roc_curves.png")
    print("   - decision_tree_analysis.png")
    print("   - classification_performance_summary.png")
    
    print("\nğŸ“ˆ ì£¼ìš” ì¸ì‚¬ì´íŠ¸:")
    print("1. ì™€ì¸ ë°ì´í„°ëŠ” í´ë˜ìŠ¤ ê°„ êµ¬ë¶„ì´ ëª…í™•í•œ í¸")
    print("2. ë¡œì§€ìŠ¤í‹± íšŒê·€: ì„ í˜• ê²°ì • ê²½ê³„, ë¹ ë¥¸ í•™ìŠµ")
    print("3. ê²°ì • íŠ¸ë¦¬: í•´ì„ ê°€ëŠ¥, ë¹„ì„ í˜• íŒ¨í„´ ìºì¹˜")
    print("4. ëœë¤ í¬ë ˆìŠ¤íŠ¸: ì•™ìƒë¸”ë¡œ ë†’ì€ ì¼ë°˜í™” ì„±ëŠ¥")
    print("5. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ìœ¼ë¡œ ì„±ëŠ¥ ê°œì„  ê°€ëŠ¥")
    
    return results, best_dt, best_rf

if __name__ == "__main__":
    results, best_dt, best_rf = main()