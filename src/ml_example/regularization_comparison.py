"""
ì‹¤ìŠµ 2: ì •ê·œí™” ê¸°ë²• ë¹„êµ (Ridge & Lasso)

ì •ê·œí™”(Regularization)ì˜ í•µì‹¬ ê°œë…:
- ê³¼ì í•©(Overfitting) ë°©ì§€ë¥¼ ìœ„í•œ ê¸°ë²•
- ì†ì‹¤ í•¨ìˆ˜ì— í˜ë„í‹°(penalty) í•­ì„ ì¶”ê°€í•˜ì—¬ ëª¨ë¸ ë³µì¡ë„ ì œì–´

ì •ê·œí™” ìœ í˜•:
1. Ridge (L2): ê°€ì¤‘ì¹˜ì˜ ì œê³±í•©ì— ë¹„ë¡€í•˜ëŠ” í˜ë„í‹°
   - ì†ì‹¤í•¨ìˆ˜ = MSE + Î± * Î£(Î²áµ¢Â²)
   - ê°€ì¤‘ì¹˜ë¥¼ 0ì— ê°€ê¹ê²Œ ë§Œë“¤ì§€ë§Œ ì™„ì „íˆ 0ìœ¼ë¡œ ë§Œë“¤ì§€ëŠ” ì•ŠìŒ
   - ëª¨ë“  í”¼ì²˜ë¥¼ ì‚¬ìš©í•˜ë˜ ì˜í–¥ë ¥ì„ ì¤„ì„

2. Lasso (L1): ê°€ì¤‘ì¹˜ì˜ ì ˆëŒ“ê°’ í•©ì— ë¹„ë¡€í•˜ëŠ” í˜ë„í‹°  
   - ì†ì‹¤í•¨ìˆ˜ = MSE + Î± * Î£|Î²áµ¢|
   - ì¼ë¶€ ê°€ì¤‘ì¹˜ë¥¼ ì •í™•íˆ 0ìœ¼ë¡œ ë§Œë“¤ì–´ í”¼ì²˜ ì„ íƒ íš¨ê³¼
   - ìë™ìœ¼ë¡œ ì¤‘ìš”í•˜ì§€ ì•Šì€ í”¼ì²˜ë¥¼ ì œê±°

3. Elastic Net: Ridge + Lasso ì¡°í•©
   - ì†ì‹¤í•¨ìˆ˜ = MSE + Î±â‚ * Î£|Î²áµ¢| + Î±â‚‚ * Î£(Î²áµ¢Â²)
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import make_regression, fetch_california_housing
from sklearn.model_selection import train_test_split, cross_val_score, validation_curve
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
import warnings
warnings.filterwarnings('ignore')

def create_synthetic_data():
    """
    ì •ê·œí™” íš¨ê³¼ë¥¼ ëª…í™•íˆ ë³´ê¸° ìœ„í•œ í•©ì„± ë°ì´í„° ìƒì„±
    - ê³ ì°¨ì› ë°ì´í„°ì—ì„œ ì •ê·œí™”ì˜ íš¨ê³¼ê°€ ë” ëª…í™•í•˜ê²Œ ë‚˜íƒ€ë‚¨
    - ì¼ë¶€ í”¼ì²˜ëŠ” ë…¸ì´ì¦ˆ, ì¼ë¶€ëŠ” ì‹¤ì œ ìœ ìš©í•œ í”¼ì²˜
    """
    print("=== í•©ì„± ë°ì´í„° ìƒì„± ===")
    
    # 1. ê¸°ë³¸ íšŒê·€ ë°ì´í„° ìƒì„±
    X, y = make_regression(
        n_samples=200,      # ìƒ˜í”Œ ìˆ˜ë¥¼ ì ê²Œ ì„¤ì •í•˜ì—¬ ê³¼ì í•© ìœ ë„
        n_features=20,      # í”¼ì²˜ ìˆ˜
        n_informative=5,    # ì‹¤ì œ ìœ ìš©í•œ í”¼ì²˜ ìˆ˜ (ë‚˜ë¨¸ì§€ëŠ” ë…¸ì´ì¦ˆ)
        noise=0.1,          # ë…¸ì´ì¦ˆ ë ˆë²¨
        random_state=42
    )
    
    feature_names = [f'feature_{i:02d}' for i in range(X.shape[1])]
    
    print(f"ë°ì´í„° í˜•íƒœ: {X.shape}")
    print(f"ìœ ìš©í•œ í”¼ì²˜: 5ê°œ, ë…¸ì´ì¦ˆ í”¼ì²˜: 15ê°œ")
    
    return X, y, feature_names

def load_real_data():
    """
    ì‹¤ì œ ë°ì´í„°(ìº˜ë¦¬í¬ë‹ˆì•„ ì£¼íƒ ë°ì´í„°)ë¡œë„ ë¹„êµ ë¶„ì„
    """
    housing = fetch_california_housing()
    return housing.data, housing.target, housing.feature_names

def compare_regularization_models(X, y, feature_names, alpha_range=None):
    """
    ë‹¤ì–‘í•œ ì •ê·œí™” ëª¨ë¸ ë¹„êµ
    - ì¼ë°˜ ì„ í˜•íšŒê·€, Ridge, Lasso, Elastic Net ì„±ëŠ¥ ë¹„êµ
    - êµì°¨ê²€ì¦ì„ í†µí•œ robustí•œ ì„±ëŠ¥ í‰ê°€
    """
    if alpha_range is None:
        alpha_range = np.logspace(-3, 2, 20)  # 0.001 ~ 100
    
    print("\n=== ì •ê·œí™” ëª¨ë¸ ë¹„êµ ===")
    
    # ë°ì´í„° ë¶„í• 
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )
    
    # ìŠ¤ì¼€ì¼ë§ (ì •ê·œí™”ì—ì„œ í•„ìˆ˜)
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # ëª¨ë¸ ì •ì˜
    models = {
        'Linear Regression': LinearRegression(),
        'Ridge': Ridge(alpha=1.0),
        'Lasso': Lasso(alpha=1.0, max_iter=1000),
        'Elastic Net': ElasticNet(alpha=1.0, l1_ratio=0.5, max_iter=1000)
    }
    
    results = {}
    
    for name, model in models.items():
        print(f"\n--- {name} ---")
        
        # ëª¨ë¸ í•™ìŠµ
        model.fit(X_train_scaled, y_train)
        
        # ì˜ˆì¸¡
        y_train_pred = model.predict(X_train_scaled)
        y_test_pred = model.predict(X_test_scaled)
        
        # ì„±ëŠ¥ ì§€í‘œ
        train_r2 = r2_score(y_train, y_train_pred)
        test_r2 = r2_score(y_test, y_test_pred)
        train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))
        test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))
        
        # êµì°¨ê²€ì¦ ì ìˆ˜ (ë” robustí•œ í‰ê°€)
        cv_scores = cross_val_score(model, X_train_scaled, y_train, 
                                   cv=5, scoring='r2')
        
        results[name] = {
            'model': model,
            'train_r2': train_r2,
            'test_r2': test_r2,
            'train_rmse': train_rmse,
            'test_rmse': test_rmse,
            'cv_mean': cv_scores.mean(),
            'cv_std': cv_scores.std()
        }
        
        print(f"  í•™ìŠµ RÂ²: {train_r2:.4f}")
        print(f"  í…ŒìŠ¤íŠ¸ RÂ²: {test_r2:.4f}")
        print(f"  CV RÂ² í‰ê· : {cv_scores.mean():.4f} (Â±{cv_scores.std():.4f})")
        print(f"  ê³¼ì í•© ì •ë„: {train_r2 - test_r2:.4f}")
        
        # ê³¼ì í•© í•´ì„: í•™ìŠµ ì ìˆ˜ì™€ í…ŒìŠ¤íŠ¸ ì ìˆ˜ì˜ ì°¨ì´ê°€ í´ìˆ˜ë¡ ê³¼ì í•©
    
    return results, (X_train_scaled, X_test_scaled, y_train, y_test)

def find_optimal_alpha(X_train, y_train, model_class, alpha_range):
    """
    êµì°¨ê²€ì¦ì„ í†µí•œ ìµœì  ì•ŒíŒŒ(ì •ê·œí™” ê°•ë„) ì°¾ê¸°
    
    ì•ŒíŒŒ ê°’ì˜ ì˜ë¯¸:
    - ì•ŒíŒŒ = 0: ì •ê·œí™” ì—†ìŒ (ì¼ë°˜ ì„ í˜•íšŒê·€ì™€ ë™ì¼)
    - ì•ŒíŒŒ â†‘: ì •ê·œí™” ê°•ë„ ì¦ê°€ (underfitting ìœ„í—˜)
    - ì•ŒíŒŒ â†“: ì •ê·œí™” ì•½í™” (overfitting ìœ„í—˜)
    """
    print(f"\n=== {model_class.__name__} ìµœì  ì•ŒíŒŒ íƒìƒ‰ ===")
    
    train_scores, valid_scores = validation_curve(
        model_class(max_iter=1000), X_train, y_train,
        param_name='alpha', param_range=alpha_range,
        cv=5, scoring='r2', n_jobs=-1
    )
    
    train_mean = train_scores.mean(axis=1)
    train_std = train_scores.std(axis=1)
    valid_mean = valid_scores.mean(axis=1)
    valid_std = valid_scores.std(axis=1)
    
    # ìµœì  ì•ŒíŒŒ: ê²€ì¦ ì ìˆ˜ê°€ ê°€ì¥ ë†’ì€ ì§€ì 
    best_alpha_idx = np.argmax(valid_mean)
    best_alpha = alpha_range[best_alpha_idx]
    
    print(f"ìµœì  ì•ŒíŒŒ: {best_alpha:.4f}")
    print(f"ìµœì  ì•ŒíŒŒì—ì„œ ê²€ì¦ RÂ²: {valid_mean[best_alpha_idx]:.4f}")
    
    return {
        'alpha_range': alpha_range,
        'train_mean': train_mean,
        'train_std': train_std,
        'valid_mean': valid_mean,
        'valid_std': valid_std,
        'best_alpha': best_alpha,
        'best_alpha_idx': best_alpha_idx
    }

def plot_validation_curves(ridge_results, lasso_results):
    """
    ì•ŒíŒŒ ê°’ì— ë”°ë¥¸ ì„±ëŠ¥ ë³€í™” ì‹œê°í™”
    - Bias-Variance Tradeoff ì‹œê°í™”
    - ê³¼ì í•©/ì–¸ë”í”¼íŒ… êµ¬ê°„ ì‹ë³„
    """
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
    
    # Ridge ê²€ì¦ ê³¡ì„ 
    ax1.plot(ridge_results['alpha_range'], ridge_results['train_mean'], 
             'o-', color='blue', label='í•™ìŠµ ì ìˆ˜')
    ax1.fill_between(ridge_results['alpha_range'], 
                     ridge_results['train_mean'] - ridge_results['train_std'],
                     ridge_results['train_mean'] + ridge_results['train_std'],
                     alpha=0.1, color='blue')
    
    ax1.plot(ridge_results['alpha_range'], ridge_results['valid_mean'],
             'o-', color='red', label='ê²€ì¦ ì ìˆ˜')
    ax1.fill_between(ridge_results['alpha_range'],
                     ridge_results['valid_mean'] - ridge_results['valid_std'],
                     ridge_results['valid_mean'] + ridge_results['valid_std'],
                     alpha=0.1, color='red')
    
    ax1.axvline(ridge_results['best_alpha'], color='green', linestyle='--', 
                label=f'ìµœì  Î± = {ridge_results["best_alpha"]:.4f}')
    ax1.set_xscale('log')
    ax1.set_xlabel('Alpha (ì •ê·œí™” ê°•ë„)')
    ax1.set_ylabel('RÂ² Score')
    ax1.set_title('Ridge íšŒê·€ - ê²€ì¦ ê³¡ì„ ')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # Lasso ê²€ì¦ ê³¡ì„ 
    ax2.plot(lasso_results['alpha_range'], lasso_results['train_mean'],
             'o-', color='blue', label='í•™ìŠµ ì ìˆ˜')
    ax2.fill_between(lasso_results['alpha_range'],
                     lasso_results['train_mean'] - lasso_results['train_std'],
                     lasso_results['train_mean'] + lasso_results['train_std'],
                     alpha=0.1, color='blue')
    
    ax2.plot(lasso_results['alpha_range'], lasso_results['valid_mean'],
             'o-', color='red', label='ê²€ì¦ ì ìˆ˜')
    ax2.fill_between(lasso_results['alpha_range'],
                     lasso_results['valid_mean'] - lasso_results['valid_std'],
                     lasso_results['valid_mean'] + lasso_results['valid_std'],
                     alpha=0.1, color='red')
    
    ax2.axvline(lasso_results['best_alpha'], color='green', linestyle='--',
                label=f'ìµœì  Î± = {lasso_results["best_alpha"]:.4f}')
    ax2.set_xscale('log')
    ax2.set_xlabel('Alpha (ì •ê·œí™” ê°•ë„)')
    ax2.set_ylabel('RÂ² Score')
    ax2.set_title('Lasso íšŒê·€ - ê²€ì¦ ê³¡ì„ ')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('regularization_validation_curves.png', dpi=300, bbox_inches='tight')
    plt.show()

def analyze_feature_selection(X, y, feature_names, alpha_range):
    """
    Lassoì˜ í”¼ì²˜ ì„ íƒ íš¨ê³¼ ë¶„ì„
    - ì•ŒíŒŒ ê°’ì— ë”°ë¼ ì–´ë–¤ í”¼ì²˜ê°€ ì œê±°ë˜ëŠ”ì§€ ì¶”ì 
    - í”¼ì²˜ ì„ íƒì˜ ì•ˆì •ì„± í‰ê°€
    """
    print("\n=== Lasso í”¼ì²˜ ì„ íƒ ë¶„ì„ ===")
    
    # ë°ì´í„° ì¤€ë¹„
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    
    # ê° ì•ŒíŒŒê°’ì— ëŒ€í•´ ëª¨ë¸ í•™ìŠµí•˜ê³  ê³„ìˆ˜ ì €ì¥
    coef_path = []
    alphas_used = []
    
    for alpha in alpha_range:
        lasso = Lasso(alpha=alpha, max_iter=1000)
        lasso.fit(X_train_scaled, y_train)
        coef_path.append(lasso.coef_)
        alphas_used.append(alpha)
    
    coef_path = np.array(coef_path)
    
    # ê° ì•ŒíŒŒì—ì„œ 0ì´ ì•„ë‹Œ í”¼ì²˜ ê°œìˆ˜
    non_zero_features = [np.sum(coef != 0) for coef in coef_path]
    
    print(f"ì•ŒíŒŒ ë²”ìœ„: {min(alphas_used):.4f} ~ {max(alphas_used):.4f}")
    print(f"ì„ íƒëœ í”¼ì²˜ ìˆ˜ ë²”ìœ„: {min(non_zero_features)} ~ {max(non_zero_features)}")
    
    # ì‹œê°í™”
    plt.figure(figsize=(15, 5))
    
    # 1. ê³„ìˆ˜ ê²½ë¡œ (Coefficient Path)
    plt.subplot(1, 2, 1)
    for i in range(len(feature_names)):
        plt.plot(alphas_used, coef_path[:, i], label=feature_names[i] if len(feature_names) <= 10 else None)
    
    plt.xscale('log')
    plt.xlabel('Alpha (ì •ê·œí™” ê°•ë„)')
    plt.ylabel('ê³„ìˆ˜ ê°’')
    plt.title('Lasso ê³„ìˆ˜ ê²½ë¡œ')
    plt.grid(True, alpha=0.3)
    if len(feature_names) <= 10:
        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    
    # 2. ì„ íƒëœ í”¼ì²˜ ê°œìˆ˜
    plt.subplot(1, 2, 2)
    plt.plot(alphas_used, non_zero_features, 'o-', color='red', linewidth=2)
    plt.xscale('log')
    plt.xlabel('Alpha (ì •ê·œí™” ê°•ë„)')
    plt.ylabel('0ì´ ì•„ë‹Œ í”¼ì²˜ ê°œìˆ˜')
    plt.title('Lasso í”¼ì²˜ ì„ íƒ')
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('lasso_feature_selection.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    return coef_path, alphas_used, non_zero_features

def compare_coefficients(results, feature_names):
    """
    ê° ëª¨ë¸ì˜ ê³„ìˆ˜ ë¹„êµ ë¶„ì„
    - ì •ê·œí™”ê°€ ê³„ìˆ˜ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ ì‹œê°í™”
    - í”¼ì²˜ ì¤‘ìš”ë„ í•´ì„
    """
    print("\n=== ëª¨ë¸ë³„ ê³„ìˆ˜ ë¹„êµ ===")
    
    # ê³„ìˆ˜ ë°ì´í„° ì¤€ë¹„ (Linear Regression, Ridge, Lassoë§Œ ë¹„êµ)
    models_to_compare = ['Linear Regression', 'Ridge', 'Lasso']
    coef_data = {}
    
    for name in models_to_compare:
        if hasattr(results[name]['model'], 'coef_'):
            coef_data[name] = results[name]['model'].coef_
    
    # DataFrame ìƒì„±
    coef_df = pd.DataFrame(coef_data, index=feature_names)
    print("\nê³„ìˆ˜ ë¹„êµ í…Œì´ë¸”:")
    print(coef_df.round(4))
    
    # ì‹œê°í™”
    plt.figure(figsize=(12, 8))
    
    # íˆíŠ¸ë§µìœ¼ë¡œ ê³„ìˆ˜ ë¹„êµ
    plt.subplot(2, 1, 1)
    sns.heatmap(coef_df.T, annot=True, cmap='RdBu_r', center=0, 
                fmt='.3f', cbar_kws={'label': 'ê³„ìˆ˜ ê°’'})
    plt.title('ëª¨ë¸ë³„ ê³„ìˆ˜ íˆíŠ¸ë§µ')
    
    # ë§‰ëŒ€ ê·¸ë˜í”„ë¡œ ê³„ìˆ˜ ë¹„êµ
    plt.subplot(2, 1, 2)
    x = np.arange(len(feature_names))
    width = 0.25
    
    for i, name in enumerate(models_to_compare):
        offset = (i - 1) * width
        plt.bar(x + offset, coef_data[name], width, label=name, alpha=0.8)
    
    plt.xlabel('í”¼ì²˜')
    plt.ylabel('ê³„ìˆ˜ ê°’')
    plt.title('ëª¨ë¸ë³„ ê³„ìˆ˜ ë¹„êµ')
    plt.xticks(x, feature_names, rotation=45, ha='right')
    plt.legend()
    plt.axhline(y=0, color='black', linestyle='-', alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('coefficient_comparison.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # Lassoì˜ ì œë¡œ ê³„ìˆ˜ í”¼ì²˜ ë¶„ì„
    if 'Lasso' in coef_data:
        zero_features = [name for name, coef in zip(feature_names, coef_data['Lasso']) if abs(coef) < 1e-10]
        if zero_features:
            print(f"\nLassoì—ì„œ ì œê±°ëœ í”¼ì²˜ ({len(zero_features)}ê°œ):")
            for feature in zero_features:
                print(f"  - {feature}")
        else:
            print("\nLassoì—ì„œ ì œê±°ëœ í”¼ì²˜ ì—†ìŒ")

def performance_summary(results):
    """
    ëª¨ë¸ ì„±ëŠ¥ ìš”ì•½ ë° ì‹œê°í™”
    """
    print("\n=== ëª¨ë¸ ì„±ëŠ¥ ìš”ì•½ ===")
    
    # ì„±ëŠ¥ ë°ì´í„° ì •ë¦¬
    summary_data = []
    for name, result in results.items():
        summary_data.append({
            'Model': name,
            'Test RÂ²': result['test_r2'],
            'CV Mean': result['cv_mean'],
            'CV Std': result['cv_std'],
            'Overfitting': result['train_r2'] - result['test_r2']
        })
    
    summary_df = pd.DataFrame(summary_data)
    print(summary_df.round(4))
    
    # ì‹œê°í™”
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
    
    # 1. RÂ² ì ìˆ˜ ë¹„êµ
    axes[0, 0].bar(summary_df['Model'], summary_df['Test RÂ²'], color='skyblue')
    axes[0, 0].set_title('í…ŒìŠ¤íŠ¸ RÂ² ì ìˆ˜')
    axes[0, 0].set_ylabel('RÂ² Score')
    axes[0, 0].tick_params(axis='x', rotation=45)
    
    # 2. êµì°¨ê²€ì¦ ì ìˆ˜ (ì—ëŸ¬ë°” í¬í•¨)
    axes[0, 1].bar(summary_df['Model'], summary_df['CV Mean'], 
                   yerr=summary_df['CV Std'], capsize=5, color='lightgreen')
    axes[0, 1].set_title('êµì°¨ê²€ì¦ RÂ² ì ìˆ˜')
    axes[0, 1].set_ylabel('RÂ² Score')
    axes[0, 1].tick_params(axis='x', rotation=45)
    
    # 3. ê³¼ì í•© ì •ë„
    colors = ['red' if x > 0.05 else 'green' for x in summary_df['Overfitting']]
    axes[1, 0].bar(summary_df['Model'], summary_df['Overfitting'], color=colors)
    axes[1, 0].axhline(y=0.05, color='red', linestyle='--', alpha=0.7, label='ê³¼ì í•© ê¸°ì¤€ì„ ')
    axes[1, 0].set_title('ê³¼ì í•© ì •ë„ (í•™ìŠµ RÂ² - í…ŒìŠ¤íŠ¸ RÂ²)')
    axes[1, 0].set_ylabel('RÂ² ì°¨ì´')
    axes[1, 0].tick_params(axis='x', rotation=45)
    axes[1, 0].legend()
    
    # 4. ì¢…í•© ì ìˆ˜ (í…ŒìŠ¤íŠ¸ RÂ² - ê³¼ì í•© í˜ë„í‹°)
    composite_score = summary_df['Test RÂ²'] - np.maximum(0, summary_df['Overfitting'] - 0.02)
    axes[1, 1].bar(summary_df['Model'], composite_score, color='orange')
    axes[1, 1].set_title('ì¢…í•© ì ìˆ˜ (í…ŒìŠ¤íŠ¸ RÂ² - ê³¼ì í•© í˜ë„í‹°)')
    axes[1, 1].set_ylabel('ì ìˆ˜')
    axes[1, 1].tick_params(axis='x', rotation=45)
    
    plt.tight_layout()
    plt.savefig('regularization_performance_summary.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì‹ë³„
    best_model = summary_df.loc[summary_df['Test RÂ²'].idxmax(), 'Model']
    most_stable = summary_df.loc[summary_df['Overfitting'].idxmin(), 'Model']
    
    print(f"\nğŸ† ìµœê³  í…ŒìŠ¤íŠ¸ ì„±ëŠ¥: {best_model}")
    print(f"ğŸ¯ ê°€ì¥ ì•ˆì •ì ì¸ ëª¨ë¸: {most_stable}")

def main():
    """
    ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
    """
    print("ğŸ“Š ì •ê·œí™” ê¸°ë²• ë¹„êµ ë¶„ì„ (Ridge vs Lasso)")
    print("=" * 60)
    
    # ì•ŒíŒŒ ë²”ìœ„ ì„¤ì •
    alpha_range = np.logspace(-4, 1, 30)
    
    print("\nğŸ”¬ í•©ì„± ë°ì´í„°ë¡œ ë¶„ì„")
    print("-" * 30)
    
    # 1. í•©ì„± ë°ì´í„° ë¶„ì„
    X_synthetic, y_synthetic, synthetic_features = create_synthetic_data()
    
    # ëª¨ë¸ ë¹„êµ
    results_synthetic, data_splits = compare_regularization_models(
        X_synthetic, y_synthetic, synthetic_features, alpha_range
    )
    
    X_train, X_test, y_train, y_test = data_splits
    
    # ìµœì  ì•ŒíŒŒ íƒìƒ‰
    ridge_validation = find_optimal_alpha(X_train, y_train, Ridge, alpha_range)
    lasso_validation = find_optimal_alpha(X_train, y_train, Lasso, alpha_range)
    
    # ê²€ì¦ ê³¡ì„  ì‹œê°í™”
    plot_validation_curves(ridge_validation, lasso_validation)
    
    # í”¼ì²˜ ì„ íƒ ë¶„ì„
    analyze_feature_selection(X_synthetic, y_synthetic, synthetic_features, alpha_range)
    
    # ê³„ìˆ˜ ë¹„êµ
    compare_coefficients(results_synthetic, synthetic_features)
    
    # ì„±ëŠ¥ ìš”ì•½
    performance_summary(results_synthetic)
    
    print("\nğŸ  ì‹¤ì œ ë°ì´í„°ë¡œ ë¶„ì„")
    print("-" * 30)
    
    # 2. ì‹¤ì œ ë°ì´í„° ë¶„ì„
    X_real, y_real, real_features = load_real_data()
    
    results_real, _ = compare_regularization_models(X_real, y_real, real_features)
    compare_coefficients(results_real, real_features)
    performance_summary(results_real)
    
    print("\n" + "=" * 60)
    print("âœ… ë¶„ì„ ì™„ë£Œ!")
    print("ğŸ“Š ìƒì„±ëœ ì‹œê°í™” íŒŒì¼:")
    print("   - regularization_validation_curves.png")
    print("   - lasso_feature_selection.png")
    print("   - coefficient_comparison.png")
    print("   - regularization_performance_summary.png")
    
    print("\nğŸ“ˆ ì£¼ìš” ì¸ì‚¬ì´íŠ¸:")
    print("1. Ridge: ëª¨ë“  í”¼ì²˜ ìœ ì§€, ê³„ìˆ˜ í¬ê¸° ì¶•ì†Œ")
    print("2. Lasso: ë¶ˆí•„ìš”í•œ í”¼ì²˜ ìë™ ì œê±°, ìŠ¤íŒŒìŠ¤ ëª¨ë¸ ìƒì„±")
    print("3. ì •ê·œí™” ê°•ë„(Î±) ì¡°ì •ìœ¼ë¡œ Bias-Variance ê· í˜• ì œì–´")
    print("4. êµì°¨ê²€ì¦ì„ í†µí•œ ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„ íƒ ì¤‘ìš”")

if __name__ == "__main__":
    main()